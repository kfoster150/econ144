---
title: "HW 2"
Date: date()
output:
  html_document:
    df_print: paged
Author: Kenneth Foster
---
```{r message=FALSE, include=FALSE}
setwd("C:\\Users\\kenny\\Desktop\\UCLA Spring 2018\\Forecasting\\Hw 2")

# To set plot margins:
par(mar=c(1,1,1,1))

rm(list=ls(all=TRUE))

library(lattice)
library(foreign)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(leaps)
library(mgcv)
library(rpart)
library(pan)
library(mgcv)
library(DAAG)
library("TTR")
library(tis)
require("datasets")
require(graphics)
library(forecast)
require(astsa)
library(RColorBrewer)
library(plotrix)
library(nlstools)
library(forecast)
library(gridExtra)
library(dynlm)
library(Hmisc)
library(tseries)
library(moments)

```

## Question 1

```{r include=FALSE}
LFPR <- read.table("labordata.dat")
vnames <- c("male", "female", "total")
names(LFPR) <- vnames
attach(LFPR)
```

# (a) Time-series plot

```{r}
total_ts <- ts(total, start = 1948, frequency = 12)
female_ts <- ts(female, start = 1948, frequency = 12)
male_ts <- ts(male, start = 1948, frequency = 12)
t<-seq(1948, 1991,length=length(total_ts))

ts.plot(male_ts, total_ts, female_ts, 
        gpars = list(col = c("red", "black", "blue")), ylab = "In Percent %", main = "Labor force participation rate")
legend('bottomright', legend = c("Male", "Total", "Female"), fill = c("red", "black", "blue"), horiz = TRUE, cex = 0.75)
```

# (b) Fit models

```{r}
# Linear Fit
m1=lm(female_ts~t)

# Polynomial fit
m2=lm(female_ts~t+I(t^2)+I(t^3))

# Exponential fit
ds=data.frame(x=t, y=female_ts)
y = c(ds$y)
x = c(ds$x)
m4=nls(y ~ exp(a + b * x), data=ds, start = list(a = 0, b = 0))

# Graphs
#par(mar=c(1,1,1,1)
par(mfrow = c(3,1))

plot(female_ts,ylab="Percent %", main = "Female Participation - Linear", xlab="Time", col='blue', xlim = c(1948,1991), ylim = c(32,58))
lines(t,m1$fit,col="red", lwd = "0.5")

plot(female_ts,ylab="Percent %", main = "Female Participation - Quadratic", xlab="Time", col='blue', xlim = c(1948,1991), ylim = c(32,58))
lines(t,m2$fit,col="red", lwd = "0.5")

plot(female_ts,ylab="Percent %", main = "Female Participation - Exponential", xlab="Time", col='blue', xlim = c(1948,1991), ylim = c(32,58))
lines(x, predict(m4), col="red")
```

# (c) Residuals vs. Fitted Values

```{r message=FALSE, warning=FALSE}
par(mfrow = c(3,1))

plot(m1$fitted.values, m1$residuals, type = "line", ylab = "residuals", xlab = paste("fitted values --- RSS =", sum((m1$residuals)^2)), main = "Female Participation - Linear")
abline(h=0, col = "red")

plot(m2$fitted.values, m1$residuals, type = "line", ylab = "residuals", xlab = paste("fitted values --- RSS =", sum((m2$residuals)^2)), main = "Female Participation - Quadratic")
abline(h=0, col = "red")

plot(predict(m4), resid(m4), type = "line", ylab = "residuals", xlab = paste("fitted values --", sum((resid(m4))^2)), main = "Female Participation - Exponential")
abline(h=0, col = "red")
```

# (d) AIC & BIC analysis

```{r message=FALSE, warning=FALSE}
aicfem <- c(AIC(m1,m2,m4))
bicfem <- c(BIC(m1,m2,m4))
fit.table <- data.frame(aicfem,bicfem)
grid.table(fit.table, rows = c("linear", "quadratic","exponential"))
```

# (e) Forecasting 

```{r}
tn=data.frame(t=seq(1992,2002))

pred.plim = predict(lm(female_ts~t+I(t^2)+I(t^3)),tn, level =0.95, interval="prediction")
pred.clim = predict(lm(female_ts~t+I(t^2)+I(t^3)), tn,level=0.95, interval="confidence")
matplot(tn$t,cbind(pred.clim, pred.plim[,-1]),
        lty=c(1,1,1,3,3), type="l", lwd=2, ylab="Percent %",xlab="Time", main = "Predicted Female Labor Participation")

#Discuss
```

# (f) Holt-Winters Model

```{r}
female.hw<-HoltWinters(female_ts)
plot(female_ts,xlab="Year", ylab="Percent %", main = "Holt-Winters Fit")
lines(female.hw$fitted[,1],col="red")

# It's perdy dang gewd

```

# (g) Holt-Winters Forecast

```{r}

plot(forecast(female_ts, h = 120, level = c(90,95,99)),main="Point & Interval Forecast",xlab="Year", ylab="percent %",xlim=c(1980,2002), ylim = c(50,70), shadecols="oldstyle", lwd = 0.5)
legend('topleft', legend = c("90%","95%","99%"), fill = c("orangered", "orange", "yellow"))

```


## Question 2; Exercise 3.2

```{r message=FALSE, warning=FALSE, include=FALSE}
exp_inc <- read.csv("exp_inc.csv", header = TRUE)
cpi_tbill <- read.csv("cpi_tbill.csv", header = TRUE)
attach(exp_inc)
attach(cpi_tbill)
```

```{r}
time <- c(seq(1959, 2011,length=636), seq(2012, 2012, length = 4))

l_exp <- log(rpce)
l_inc <- log(rdpi)

GrowthExp <- diff(l_exp)
GrowthInc <- diff(l_inc)

GrowthExp <- c(0, GrowthExp)
GrowthInc <- c(0, GrowthInc)

reg1 <- lm(GrowthExp~GrowthInc)

inf <- diff(log(cpi))
inf <- c(0, inf)
R_Tbill <- (nomrate - inf)

reg2 <- lm(GrowthExp~GrowthInc+R_Tbill)

summary(reg1)
summary(reg2)

AIC(reg1,reg2)
BIC(reg1,reg2)

plot(time, GrowthExp, type = "l")
lines(time, reg1$fitted.values, col = "blue")
lines(time, reg2$fitted.values, col = "red")
legend('bottomleft', legend = c("Consumption Growth", "Income Growth Fitted values", "Income Growth + Real Interest Rates Fitted Values"), fill = c("black", "blue", "red"))
```

Real interest rates don't significantly improve the model. R-squared, AIC, and BIC values are very similar for both models. The estimates for the effects are economically and statistically insignificant.


# Question 3; Exercise 4.4

```{r message=FALSE, warning=FALSE}
houseann <- read.csv("housepricesann.csv", header = TRUE)
houseqtr <- read.csv("houseprices.csv", header = TRUE)
attach(houseann)
attach(houseqtr)

ann_price <- ts(houseann$P, start = 1975, end = 2011)
qtr_price <- ts(houseqtr$P, start = c(1980,2), end = c(2011,3), frequency = 4)

ann_growth <- diff(log(ann_price))
qtr_growth <- diff(log(qtr_price))

ha1 <- lm(ann_growth ~ Lag(ann_growth, -1))
ha2 <- lm(ann_growth ~ Lag(ann_growth, -1) + Lag(ann_growth, -2))
ha3 <- lm(ann_growth ~ Lag(ann_growth, -1) + Lag(ann_growth, -2)+Lag(ann_growth,-3))
ha4 <- lm(ann_growth ~ Lag(ann_growth, -1) + Lag(ann_growth, -2)+Lag(ann_growth,-3) + Lag(ann_growth, -4))

hq1 <- lm(qtr_growth ~ Lag(qtr_growth, -1))
hq2 <- lm(qtr_growth ~ Lag(qtr_growth, -1) + Lag(qtr_growth, -2))
hq3 <- lm(qtr_growth ~ Lag(qtr_growth, -1) + Lag(qtr_growth, -2)+Lag(qtr_growth,-3))
hq4 <- lm(qtr_growth ~ Lag(qtr_growth, -1) + Lag(qtr_growth, -2)+Lag(qtr_growth,-3) + Lag(qtr_growth, -4))

aichprice <- c(AIC(ha1, ha2, ha3, ha4, hq1, hq2, hq3, hq4))
bichprice <- c(BIC(ha1, ha2, ha3, ha4, hq1, hq2, hq3, hq4))

fit.table2 <- data.frame(aichprice,bichprice)
grid.table(fit.table2, rows = c("Ann. Lag 1", "Ann. Lag 2", "Ann. Lag 3", "Ann. Lag 4", "Qtr. Lag 1", "Qtr. Lag 2", "Qtr. Lag 3", "Qtr. Lag 4"))

summary(ha1)
summary(ha2)

jarque.bera.test(ha1$residuals)
jarque.bera.test(ha2$residuals)

# Recursive: use allll the data (gotta figure out tomorrow?)

# Rolling: use fixed window of data

# Practice using training and test set

ann_growth2 <- window(ann_growth, start = 1976, end = 1999)
ann_growth3 <- window(ann_growth, start = 2000)

ha2_fit <- meanf(ann_growth2, h=12)
ha2_fit2 <- rwf(ann_growth2, h=12)

accuracy(ha2_fit,ann_growth3)
accuracy(ha2_fit2,ann_growth3)

#know what accuracy thingies mean

```

t<-seq(1975, 2011,length=length(houseann$Year))
k <- 2 * (2011-1975)/3
n <- length(houseann$Year)

for (i in 1:k)
{
  window <- window(ann_growth, end=1975+i)
  nextwindow <- window(ann_growth, start=i, end=1976+i)
  ha2window <- lm(window ~ Lag(window, -1) + Lag(window, -2))
  ha2rollingforecast <- forecast(ha2window, h = (n/3))
  RollingCoef[i,] <- as.data.frame( (abs(ha2rollingforecast[['mean']]-start)))
}
```

Out of the 8 models for quarterly and annual price growth, annual growth with lag 1 and lag 2 have the lowest AIC and BIC values. These values are very similar for both 1 and 2 lag periods, and both have similar values of R squared. However, the model with 2 lag periods has normally distributed residuals, while the other does not. I will use annual growth with 2 lag periods.

# Question 4; Exercise 4.6
```{r}
greenbook <- read.csv("greenbook.csv", header = TRUE)
attach(greenbook)

gdpgrowth_ts <- ts(RGDPGrowth_perc, 1969, 2006, frequency = 4)
growthforecast_ts <- ts(GrowthForecast_perc, 1969, 2006, frequency = 4)

error_ts <- growthforecast_ts - gdpgrowth_ts

plot(gdpgrowth_ts, main = "GDP Growth", ylab = "")
lines(growthforecast_ts, col = "forestgreen", lwd = 2, ylim = c(-3,4))
lines(error_ts, col = "red", ylim = c(-3,4), lty = 2)
legend('topright', legend = c("Actual GDP Growth", "Greenbook Forecasts", "Forecast errors"), fill = c("black", "forestgreen", "red"))
plot.new()

# WHAT DO I OBSERVE HMMMMMM

sum.1 <- c(summary(gdpgrowth_ts), sd(gdpgrowth_ts), skewness(gdpgrowth_ts), kurtosis(gdpgrowth_ts))

sum.2 <- c(summary(growthforecast_ts), sd(growthforecast_ts), skewness(growthforecast_ts), kurtosis(growthforecast_ts))

sum.3 <- c(summary(error_ts), sd(error_ts), skewness(error_ts), kurtosis(error_ts))

sum.names <- c("min", "1st Qu", "Median", "Mean", "3rd Qu", "Max", "Std. Dev.", "Skewness", "Kurtosis")
names(sum.1) <- sum.names
names(sum.2) <- sum.names
names(sum.3) <- sum.names
descript <- data.frame(sum.1,sum.2,sum.3)

grid.table(descript, cols = c("Growth", "Forecast", "Errors"))

par(mfrow= c(2,1))
acf(gdpgrowth_ts, xlim = c(0,5))
pacf(gdpgrowth_ts, xlim = c(0,5))

par(mfrow= c(2,1))
acf(growthforecast_ts, xlim = c(0,5))
pacf(growthforecast_ts, xlim = c(0,5))

par(mfrow= c(2,1))
acf(error_ts, xlim = c(0,5))
pacf(error_ts, xlim = c(0,5))
```

GDP Growth shows 20-30% negative partial autocorrelation with 3 periods of lag: In plain english, GDP growth in one season predicts it will decline in the next season (3 month intervals)

# Question 5; Exercise 5.4
```{r}
sp_amex <- read.csv("sp_amex.csv", header = TRUE)
attach(sp_amex)

amex <- ts(AMEX, frequency = 260, start = c(1997, 257), end = c(2012,141))
sp500 <- ts(SP500, frequency = 260, start = c(1990, 22), end = c(2012, 141))

par(mfrow=c(2,2))
acf(amex)
pacf(amex)
acf(sp500)
pacf(sp500)

```

Like the plots in figure 5.6 of the textbook, display strong, statistically significant autocorrelation only at the first period of lag, then no correlation at higher values of lag. This suggests that AMEX and the S&P 500 are Moving Average processes of order 1.

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
